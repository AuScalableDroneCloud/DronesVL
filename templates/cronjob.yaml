kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: cron-runner
rules:
- apiGroups: [""] #Core api group
  #Allow deleting pods
  resources: ["pods"]
  verbs: ["get", "list", "delete"] #, "create", "patch", "get", "update", "list"]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1 
metadata:
  name: cron-runner
  namespace: default
subjects:
- kind: ServiceAccount
  name: sa-cron-runner
  namespace: default
roleRef:
  kind: Role
  name: cron-runner
  apiGroup: ""
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: sa-cron-runner
  namespace: default
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: check-website
  annotations:
    botkube.io/disable: "true"
spec:
  schedule: "*/10 * * * *" #10 min
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 2
  concurrencyPolicy: Forbid
  jobTemplate:
    metadata:
      annotations:
        botkube.io/disable: "true"
    spec:
      template:
        metadata:
          annotations:
            botkube.io/disable: "true"
        spec:
          serviceAccountName: sa-cron-runner
          containers:
          - name: check-website
            image: djieno/alpine-curl-kubectl:amd64
            command: ["/bin/sh", "/check_site.sh"]
            volumeMounts:
            - name: check-site
              mountPath: /check_site.sh
              subPath: check_site.sh
          restartPolicy: Never
          volumes:
          #ConfigMap containing check_site.sh script
          - name: check-site
            configMap:
              name: cron-script-configmap
              items:
              - key: check_site.sh
                path: check_site.sh

---
#Check script
apiVersion: v1
kind: ConfigMap
metadata:
  name: cron-script-configmap
  namespace: default
data:
  check_site.sh: |
    #!/bin/sh
    echo Checking: ${WEBAPP_HOST}

    #Check status of each replica pod, if any containers in error state then delete!
    kubectl get pods -l app=webapp-worker -o custom-columns="POD:metadata.name,STATE:status.containerStatuses[*].state.waiting.reason" --no-headers | while read name state
    do
        echo "NAME $podname STATE $podstate"
        if [ '$podstate' = 'CrashLoopBackOff' ]
        then
          echo "$podname failing, deleting pod"
          kubectl delete pods $podname --grace-period=0 --force
        else
          echo "$podname running fine"
        fi
    done

    #if [ $(kubectl get pods webapp-worker-0 -o 'jsonpath={..status.conditions[?(@.type=="Ready")].status}') = 'False' ]; then
    #  echo "POD 0 NOT READY, deleting";
    #  kubectl delete pods webapp-worker-0 --grace-period=0 --force
    #fi
    #if [ $(kubectl get pods webapp-worker-1 -o 'jsonpath={..status.conditions[?(@.type=="Ready")].status}') = 'False' ]; then
    #  echo "POD 1 NOT READY, deleting";
    #  kubectl delete pods webapp-worker-1 --grace-period=0 --force
    #fi

    #if ! nc -zv -w 10 ${WEBAPP_HOST} 443; then
    #if ! timeout -t 30 curl -sL https://${WEBAPP_HOST}/login > /dev/null; then
    #  echo "Site appears down, retrying 1"
    #  if ! timeout -t 30 curl -sL https://${WEBAPP_HOST}/login > /dev/null; then
    #    echo "Site appears down, retrying 2"
    #    if ! timeout -t 30 curl -sL https://${WEBAPP_HOST}/login > /dev/null; then
    #      echo "Site still appears down, restarting pods"
    #      kubectl delete pods webapp-worker-0 --grace-period=0 --force
    #      kubectl delete pods webapp-worker-1 --grace-period=0 --force
    #    fi
    #  fi
    #else
    #  echo "Everything is ok"
    #fi
